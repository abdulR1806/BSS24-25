{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "ecUzQKCQ0xyk",
        "UMmNyRxIDfeI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulR1806/BSS24-25/blob/main/Google_News_Scraping_v_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "ecUzQKCQ0xyk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSuyDTk_cxQf"
      },
      "outputs": [],
      "source": [
        "!pip install selenium gnews GoogleNews==1.6.13 free-proxy googlesearch-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install chromium-driver"
      ],
      "metadata": {
        "id": "TrxbRKGgpiTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! export | grep  HTTPS_PROXY"
      ],
      "metadata": {
        "id": "SJejLAp4SSTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fungsi untuk setup proxy dan chromedriver"
      ],
      "metadata": {
        "id": "BSkR4BYFqFnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from fp.fp import FreeProxy\n",
        "\n",
        "# def get_proxy(country=['US', 'BR', 'ID']):\n",
        "#   proxy = None\n",
        "\n",
        "#   # Loop until we got proxy and proxy is different from before\n",
        "#   while proxy is None:\n",
        "#     try:\n",
        "#       proxy = FreeProxy(country_id=country, timeout=0.5, https=True, rand=True).get()\n",
        "#     except:\n",
        "#       pass\n",
        "\n",
        "#   return proxy\n",
        "\n",
        "# def check_proxy(proxy, list_proxy):\n",
        "\n",
        "#   if proxy == None:\n",
        "#     proxy = get_proxy()\n",
        "#   else:\n",
        "#     while proxy in list_proxy:\n",
        "#       proxy = get_proxy()\n",
        "\n",
        "#   list_proxy.append(proxy)\n",
        "#   return proxy"
      ],
      "metadata": {
        "id": "pl_TuBkGN3JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from selenium import webdriver\n",
        "\n",
        "# def web_driver():\n",
        "#   chrome_options = webdriver.ChromeOptions()\n",
        "#   chrome_options.add_argument('--headless')\n",
        "#   chrome_options.add_argument('--no-sandbox')\n",
        "#   chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "#   chrome_options.add_argument('--disable-gpu')\n",
        "#   chrome_options.add_argument('--verbose')\n",
        "#   chrome_options.add_argument('--window-size=1920,1200')\n",
        "#   chrome_options.add_argument('--disable-extensions')\n",
        "#   driver = webdriver.Chrome( options=chrome_options)\n",
        "#   return driver"
      ],
      "metadata": {
        "id": "qr99FGv_qNBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "from google.colab import files\n",
        "from gnews import GNews\n",
        "from GoogleNews import GoogleNews\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "hqDwDYEqhK6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Function"
      ],
      "metadata": {
        "id": "UMmNyRxIDfeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(id: str, gid: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches data from a specified Google Sheets document and sheet as a pandas DataFrame.\n",
        "\n",
        "    This function constructs a URL to access a Google Sheets document in CSV format by using\n",
        "    the document's ID and the specific sheet's GID. It then reads the CSV data into a pandas DataFrame\n",
        "    and returns this DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - id (str): The unique identifier for the Google Sheets document.\n",
        "    - gid (str): The unique identifier for the specific sheet within the document.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The data from the specified Google Sheet and sheet as a pandas DataFrame.\n",
        "\n",
        "    Example:\n",
        "    >>> data_df = get_data('1a2B3cD4E5fG6hI7J8K9L0MnOpQrStUvW', '1234567890')\n",
        "    # This would fetch the data from the Google Sheet with the provided ID and GID.\n",
        "\n",
        "    Note:\n",
        "    - Ensure that the Google Sheet is publicly accessible or shared with the client service account\n",
        "      if using Google Cloud authentication, to avoid access denied errors.\n",
        "    \"\"\"\n",
        "    if not id or not gid:\n",
        "        raise ValueError(\"Google Sheet ID and GID must be provided.\")\n",
        "\n",
        "    url = f\"https://docs.google.com/spreadsheets/d/{id}/export?format=csv&gid={gid}\"\n",
        "    try:\n",
        "        data = pd.read_csv(url)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to fetch data from Google Sheets: {e}\")\n",
        "        raise\n",
        "    return data\n",
        "\n",
        "def transform_keyword(all_debitur: pd.DataFrame, keyword_filter: pd.DataFrame, filter_ket: bool=False) -> dict:\n",
        "    \"\"\"\n",
        "    Transforms the `all_debitur` DataFrame by merging it with `keyword_filter` DataFrame,\n",
        "    and modifies the 'Name & Company' column by appending the 'Filter Keyword'.\n",
        "\n",
        "    Args:\n",
        "        all_debitur (pd.DataFrame): A DataFrame containing debtor information.\n",
        "        keyword_filter (pd.DataFrame): A DataFrame containing filter keywords associated with segments.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where the keys are unique segments and the values are DataFrames\n",
        "              filtered by each segment.\n",
        "    \"\"\"\n",
        "    # Validate input DataFrames\n",
        "    required_columns_debitur = {'Segment', 'Name & Company'}\n",
        "    required_columns_filter = {'Segment', 'Filter Keyword'}\n",
        "\n",
        "    if not required_columns_debitur.issubset(all_debitur.columns):\n",
        "        error_msg = f\"all_debitur DataFrame must contain columns: {required_columns_debitur}\"\n",
        "        logging.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    if not required_columns_filter.issubset(keyword_filter.columns):\n",
        "        error_msg = f\"keyword_filter DataFrame must contain columns: {required_columns_filter}\"\n",
        "        logging.error(error_msg)\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    # Merge the two DataFrames on the 'Segment' column, filling missing values with a space\n",
        "    combined_deb = pd.merge(all_debitur, keyword_filter, how=\"outer\", on=[\"Segment\"]).fillna(\"\")\n",
        "\n",
        "    if combined_deb.empty:\n",
        "        warning_msg = \"The merged DataFrame is empty. Check input DataFrames for proper data.\"\n",
        "        logging.warning(warning_msg)\n",
        "\n",
        "    # Transform 'Name & Company' column\n",
        "    combined_deb['Name & Company'] = '\"' + combined_deb['Name & Company'] + '\"'\n",
        "    combined_deb['Name & Company'] = combined_deb['Name & Company'] + \" \" + combined_deb['Filter Keyword']\n",
        "    if filter_ket:\n",
        "      combined_deb['Name & Company'] = combined_deb['Name & Company'] + \" \" + combined_deb['Keterangan']\n",
        "\n",
        "    # Create a dictionary with unique segments as keys and filtered DataFrames as values\n",
        "    segment_dict = {segment: combined_deb[combined_deb.Segment == segment][\"Name & Company\"].to_list() for segment in combined_deb.Segment.unique()}\n",
        "\n",
        "    return segment_dict"
      ],
      "metadata": {
        "id": "RZWfuhaCVFzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_news_data(result_data: dict, type_deb: str, filter: bool = False, keyword_period: str = \"\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transforms the news data from JSON format to a pandas DataFrame and applies optional filtering.\n",
        "\n",
        "    This function processes a dictionary of news data, normalizes it into a DataFrame, and optionally filters\n",
        "    the data based on a specified keyword period. It also ensures URL formats are corrected and columns are reordered.\n",
        "\n",
        "    Parameters:\n",
        "    - result_data (dict): Dictionary containing news data with keywords as keys.\n",
        "    - type_deb (str): Type of the entity being checked.\n",
        "    - filter (bool, optional): Whether to apply filtering on the 'date' column (default is False).\n",
        "    - keyword_period (str, optional): The keyword period for filtering the 'date' column (default is \"\").\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The transformed and optionally filtered news data as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for debitur, news_items in result_data.items():\n",
        "        df = pd.json_normalize(news_items)\n",
        "        df[\"keyword\"] = debitur\n",
        "        data.append(df)\n",
        "\n",
        "    if not data:\n",
        "        logging.warning(\"No data to transform.\")\n",
        "        return pd.DataFrame(columns=['keyword', 'title', 'date', 'link', 'media'])\n",
        "\n",
        "    result = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    try:\n",
        "        result['link'] = result['link'].apply(lambda x: x.split(\"&ved=\")[0])\n",
        "    except KeyError as e:\n",
        "        logging.error(f\"Error processing links: {e}\")\n",
        "        result = result.rename(columns={\"title\": \"title\", \"url\": \"link\", \"published date\": \"date\", \"publisher.title\": \"media\"}, errors=\"raise\")\n",
        "        result['link'] = result['link'].apply(lambda x: x.split(\"&ved=\")[0])\n",
        "\n",
        "    cols = result.columns.tolist()\n",
        "    result = result[cols[-1:] + cols[:-1]]\n",
        "\n",
        "    if filter:\n",
        "        result = result[result['date'].str.contains(keyword_period)]\n",
        "        result = result[~result['date'].str.contains(\"2023\")]\n",
        "\n",
        "    result = result[['keyword', 'title', 'date', 'link', 'media']]\n",
        "    result = result.drop_duplicates(subset=['title'])\n",
        "    return result\n",
        "\n",
        "def create_excel_news(type_deb: str, df_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Creates an Excel file from the provided dataframes and returns the filename.\n",
        "\n",
        "    This function exports the cleaned and processed data to an Excel file with sheets named according to\n",
        "    the keys in the provided dictionary. The filename includes the current date and time to ensure uniqueness.\n",
        "\n",
        "    Parameters:\n",
        "    - type_deb (str): The type of entity being checked.\n",
        "    - df_dict (dict): Dictionary containing dataframes to be written to Excel, with keys as sheet names.\n",
        "\n",
        "    Returns:\n",
        "    - str: The filename of the created Excel file.\n",
        "    \"\"\"\n",
        "    now = datetime.datetime.now(pytz.timezone('Asia/Jakarta'))\n",
        "    name_file = f'news_{type_deb}_{now.strftime(\"%Y-%m-%d %H:%M:%S\")}.xlsx'\n",
        "\n",
        "    try:\n",
        "        with pd.ExcelWriter(name_file) as writer:\n",
        "            for name, df in df_dict.items():\n",
        "                df.to_excel(writer, sheet_name=name, index=False)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create Excel file: {e}\")\n",
        "        raise\n",
        "\n",
        "    return name_file\n",
        "\n",
        "def web_checking(period_check: str, type_deb: str, keyword_list: list, time_sleep: int, check_wholesale: bool = False, start: int = 1, end: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a web check for news articles and search results based on given keywords.\n",
        "\n",
        "    This function uses Google News and Google Search to fetch news articles and search results\n",
        "    for a list of keywords. It then processes the results, logs the search activities, and\n",
        "    saves the results into an Excel file.\n",
        "\n",
        "    Parameters:\n",
        "    - period_check (str): The time period to check for news articles (e.g., '14d' for 14 days).\n",
        "    - type_deb (str): The type of entity being checked (e.g., 'esme').\n",
        "    - keyword_list (list): A list of keywords to search for.\n",
        "    - time_sleep (int): The time to sleep between searches in seconds.\n",
        "    - check_wholesale (bool, optional): Whether to perform a wholesale check (default is False).\n",
        "    - start (int, optional): The starting index for the keyword list (default is 1).\n",
        "    - end (int, optional): The ending index for the keyword list (default is None, which means the end of the list).\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame containing the combined results from the searches.\n",
        "    \"\"\"\n",
        "    all_news = {}\n",
        "    all_search = {}\n",
        "    list_deb = []\n",
        "    total_news = []\n",
        "\n",
        "    googlenews = GNews(language='id', country='ID', period=period_check, max_results=100)\n",
        "    googlesearch = GoogleNews(lang='ID', period=period_check, region=\"ID\", encode='utf-8')\n",
        "\n",
        "    start_list = start - 1\n",
        "    end_list = end - 1 if end else len(keyword_list) - 1\n",
        "\n",
        "    for i in range(start_list, end_list + 1):\n",
        "        debitur = keyword_list[i]\n",
        "        news = googlenews.get_news(debitur)\n",
        "\n",
        "        if news:\n",
        "            all_news[debitur] = news\n",
        "            type_source = \"google news\"\n",
        "        else:\n",
        "            googlesearch.clear()\n",
        "            googlesearch.search(debitur)\n",
        "            news = googlesearch.results(sort=True)\n",
        "            all_search[debitur] = news\n",
        "            type_source = \"google search\"\n",
        "\n",
        "            if time_sleep < 10 or len(news) >= 10:\n",
        "                try:\n",
        "                    random_time = random.randint(1, len(news)) + 10\n",
        "                    time.sleep(random_time)\n",
        "                except:\n",
        "                    time.sleep(10)\n",
        "\n",
        "        list_deb.append(debitur)\n",
        "        total_news.append(len(news))\n",
        "        print(f\"{i+1} - {type_deb} :: berita {debitur} sudah ditambahkan dari {type_source}. Jumlah berita: {len(news)}\")\n",
        "        time.sleep(time_sleep)\n",
        "\n",
        "    log_data = {\"type_deb\": type_deb, \"keyword\": list_deb, \"hasil_pencarian\": total_news}\n",
        "    log_df = pd.DataFrame(log_data)\n",
        "\n",
        "    try:\n",
        "        df_news = transform_news_data(all_news, type_deb=type_deb, filter=False)\n",
        "        df_news['source'] = \"google_news\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to transform news data: {e}\")\n",
        "        df_news = pd.DataFrame(columns=['keyword', 'title', 'date', 'link', 'media'])\n",
        "\n",
        "    try:\n",
        "        df_search = transform_news_data(all_search, type_deb=type_deb, filter=not check_wholesale, keyword_period=\"0 bulan|minggu|hari|kemarin|jam|menit|Yesterday|ago\")\n",
        "        df_search['source'] = \"google_search\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to transform search data: {e}\")\n",
        "        df_search = pd.DataFrame(columns=['keyword', 'title', 'date', 'link', 'media'])\n",
        "\n",
        "    final_result = pd.concat([df_news, df_search], ignore_index=True)\n",
        "    final_result[\"Type\"] = type_deb\n",
        "\n",
        "    final_dict = {\"Web Checking\": final_result, \"Logs\": log_df}\n",
        "\n",
        "    try:\n",
        "        file_excel = create_excel_news(type_deb, final_dict)\n",
        "        files.download(file_excel)\n",
        "        print(f\"File excel sudah dibuat untuk debitur {type_deb}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create or download Excel file: {e}\")\n",
        "        raise\n",
        "\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "Qk4U4390G5EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi untuk legal checking"
      ],
      "metadata": {
        "id": "LgEfPPEFgRmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def get_table_case(table: str, period: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes the HTML table into a pandas DataFrame and filters it based on the given period.\n",
        "\n",
        "    Parameters:\n",
        "    - table (str): HTML content of the table.\n",
        "    - period (str): Date string to filter the cases.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Filtered DataFrame containing case information.\n",
        "    \"\"\"\n",
        "    df_list = pd.read_html(table, header=0, extract_links=\"body\")\n",
        "    df = df_list[0]\n",
        "    df.columns = df.columns.droplevel(1)\n",
        "\n",
        "    # Extract data and links separately\n",
        "    df = df.applymap(lambda x: x[0] if isinstance(x, tuple) else x)\n",
        "    df[\"Link\"] = df_list[0][\"Link\"].apply(lambda x: x[1] if isinstance(x, tuple) else '')\n",
        "\n",
        "    # Convert 'Tanggal Register' to datetime and filter based on period\n",
        "    df[\"Tanggal Register\"] = pd.to_datetime(df[\"Tanggal Register\"], format='%d %b %Y')\n",
        "    df.set_index(\"No\", inplace=True)\n",
        "    filtered_df = df.query(f\"`Tanggal Register` > '{period}'\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def get_total_case(url: str, keyword: str, period: str):\n",
        "    \"\"\"\n",
        "    Fetches the total number of cases and the detailed case data based on the keyword and period.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL to fetch data from.\n",
        "    - keyword (str): The keyword to search for.\n",
        "    - period (str): The period to filter the cases.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Total number of cases and a DataFrame of the case details.\n",
        "    \"\"\"\n",
        "    driver = web_driver()  # Ensure you have the correct WebDriver path configured\n",
        "    keyword = keyword.replace('\"', '').replace(\"'\", '')\n",
        "\n",
        "    try :\n",
        "      driver.get(url)\n",
        "      search_box = driver.find_element(By.XPATH, '//input[@name=\"search_keyword\"]')\n",
        "      search_box.send_keys(keyword)\n",
        "      search_box.submit()\n",
        "\n",
        "      result_text = driver.find_element(By.XPATH, '//div[@class=\"total_perkara\"]').text\n",
        "      total_perkara = int(re.search(r'Total : (\\d+)', result_text).group(1))\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching webpage for link '{url}': {e}\")\n",
        "\n",
        "    if total_perkara > 0:\n",
        "        table_html = driver.find_element(By.XPATH, '//table[@id=\"tablePerkaraAll\"]').get_attribute('outerHTML')\n",
        "        data = get_table_case(table=table_html, period=period)\n",
        "        print(f\"{url} :: {keyword} :: {data.shape[0]} results from {total_perkara} cases\")\n",
        "\n",
        "        return total_perkara, data\n",
        "    else:\n",
        "        print(f\"{url} :: {keyword} :: 0 results from {total_perkara} cases\")\n",
        "\n",
        "        empty_df = pd.DataFrame(columns=['Nomor Perkara', 'Tanggal Register', 'Klasifikasi Perkara', 'Para Pihak', 'Status Perkara', 'Lama Proses', 'Link'])\n",
        "        return total_perkara, empty_df\n",
        "\n",
        "def legal_checking(period_filter: str, list_url_sipp: list, keyword_list: list, time_sleep: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a legal check for cases based on given keywords and period.\n",
        "\n",
        "    Parameters:\n",
        "    - period_filter (str): The date period to filter the cases.\n",
        "    - url (str): The URL to fetch data from.\n",
        "    - keyword_list (list): A list of keywords to search for.\n",
        "    - time_sleep (int): The time to sleep between searches in seconds.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame containing the combined results from the searches.\n",
        "    \"\"\"\n",
        "    all_df = []\n",
        "    len_df = []\n",
        "    list_url = []\n",
        "    total_perkara = []\n",
        "    list_keyword = []\n",
        "\n",
        "    for url in list_url_sipp:\n",
        "\n",
        "      for keyword in keyword_list:\n",
        "          try:\n",
        "              total, data = get_total_case(url=url, keyword=keyword, period=period_filter)\n",
        "          except Exception as e:\n",
        "              logging.error(f\"Error fetching data for keyword '{keyword}': {e}\")\n",
        "              continue\n",
        "\n",
        "          list_url.append(url)\n",
        "          list_keyword.append(keyword)\n",
        "          total_perkara.append(total)\n",
        "          all_df.append(data)\n",
        "          len_df.append(data.shape[0])\n",
        "\n",
        "          time.sleep(time_sleep)\n",
        "\n",
        "    # Create log DataFrame\n",
        "    log_data = {\"url\": list_url, \"keyword\": list_keyword, \"hasil pencarian\": total_perkara, \"hasil filter\": len_df}\n",
        "    log_df = pd.DataFrame(log_data)\n",
        "\n",
        "    # Concatenate all dataframes in the 'data' list into a single dataframe\n",
        "    final_result = pd.concat(all_df, ignore_index=True)\n",
        "    final_dict = {\"Legal Checking\": final_result, \"Logs\": log_df}\n",
        "\n",
        "    try:\n",
        "        file_excel = create_excel_news(type_deb=\"legal_checking\", df_dict=final_dict)\n",
        "        files.download(file_excel)\n",
        "        print(\"File excel sudah dibuat untuk legal checking debitur\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to create or download Excel file: {e}\")\n",
        "        raise\n",
        "\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "T10iV70gIB3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_proxy = []\n",
        "proxy = None"
      ],
      "metadata": {
        "id": "XaNi3T5ccFZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Checking & Legal Checking per Debitur"
      ],
      "metadata": {
        "id": "TRnTXE08uvH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link Untuk Menambahkan kata kunci debitur : https://docs.google.com/spreadsheets/d/1fyKw095hZ6OSJeg0ZVN_lbUF1qlT2MHjNEOJXRVjTIk/edit?gid=0#gid=0"
      ],
      "metadata": {
        "id": "OuNQ_dDch4i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_sheet = \"1fyKw095hZ6OSJeg0ZVN_lbUF1qlT2MHjNEOJXRVjTIk\"\n",
        "\n",
        "all_debitur = get_data(id=id_sheet, gid=\"0\")\n",
        "keyword_filter = get_data(id=id_sheet, gid = \"395481292\")\n",
        "\n",
        "dict_all_debtors = transform_keyword(all_debitur, keyword_filter)"
      ],
      "metadata": {
        "id": "wZej3rbQV0bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_all_debtors.keys()"
      ],
      "metadata": {
        "id": "2virm9PvXiml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan Legal Checking"
      ],
      "metadata": {
        "id": "5wevvTBxltEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all_debitur = get_data(id=id_sheet, gid=\"0\")\n",
        "\n",
        "# list_wholesale_legal = all_debitur.loc[all_debitur.Segment==\"Wholesale\"]['Name & Company'].tolist()\n",
        "# list_sme_legal = all_debitur.loc[all_debitur.Segment==\"SME\"]['Name & Company'].tolist()\n",
        "# list_esme_legal = all_debitur.loc[all_debitur.Segment==\"ESME\"]['Name & Company'].tolist()\n",
        "# list_fi_legal = all_debitur.loc[all_debitur.Segment==\"FI\"]['Name & Company'].tolist()"
      ],
      "metadata": {
        "id": "4a91SJgpmhEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# period_filter=\"2024-11-01\"\n",
        "# sipp_urls = [\"https://sipp.pn-jakartaselatan.go.id/list_perkara/search\", \"https://sipp.pn-jakartabarat.go.id/list_perkara/search\",\"https://sipp.pn-jakartatimur.go.id/list_perkara/search\",\"https://sipp.pn-jakartautara.go.id/list_perkara/search\"]\n",
        "# # sipp_urls = [\"https://sipp.pn-jakartaselatan.go.id/list_perkara/search\", \"https://sipp.pn-jakartabarat.go.id/list_perkara/search\",\"https://www.pn-negara.go.id/\",\"https://www.mahkamahagung.go.id/id\"\n",
        "# time_sleep = 10\n",
        "# result = []"
      ],
      "metadata": {
        "id": "2HiueUyKlv1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"##############Legal Checking untuk Debitur Wholesale##############\")\n",
        "# wholesale_result = legal_checking(period_filter=period_filter, list_url_sipp=sipp_urls, keyword_list=list_wholesale_legal, time_sleep=time_sleep)\n",
        "# time.sleep(120)\n",
        "# print(\"##############Legal Checking untuk Debitur SME##############\")\n",
        "# sme_result = legal_checking(period_filter=period_filter, list_url_sipp=sipp_urls, keyword_list=list_sme_legal, time_sleep=time_sleep)\n",
        "# time.sleep(120)\n",
        "# print(\"##############Legal Checking untuk Debitur ESME##############\")\n",
        "# esme_result = legal_checking(period_filter=period_filter, list_url_sipp=sipp_urls, keyword_list=list_esme_legal, time_sleep=time_sleep)\n",
        "# time.sleep(120)\n",
        "# print(\"##############Legal Checking untuk Debitur FI##############\")\n",
        "# fi_result = legal_checking(period_filter=period_filter, list_url_sipp=sipp_urls, keyword_list=list_fi_legal, time_sleep=time_sleep)"
      ],
      "metadata": {
        "id": "CtE9pGdGa3Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Legal Checking per Debitur"
      ],
      "metadata": {
        "id": "z-_nm0fnhKAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# legal_checking(period_filter=period_filter, list_url_sipp=sipp_urls, keyword_list=[\"Santoso Halim\",\"Sukoco Halim\"], time_sleep=time_sleep)"
      ],
      "metadata": {
        "id": "PhOQiFNBYhD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan Web Checking"
      ],
      "metadata": {
        "id": "H4xblHG2Xec0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Checking\n",
        "df_esme = web_checking(period_check=\"14d\", type_deb = \"ESME\", keyword_list=dict_all_debtors[\"ESME\"], start=1, end=50, time_sleep=30)"
      ],
      "metadata": {
        "id": "KMGm7HOqRfib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Checking\n",
        "time.sleep(300)\n",
        "df_esme = web_checking(period_check=\"14d\", type_deb = \"ESME\", keyword_list=dict_all_debtors[\"ESME\"], start=51, time_sleep=30)"
      ],
      "metadata": {
        "id": "OvRt6GZ2RgHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300)\n",
        "df_fi = web_checking(period_check=\"14d\", type_deb = \"FI\", keyword_list=dict_all_debtors[\"FI\"], start=1, end=50, time_sleep=30)"
      ],
      "metadata": {
        "id": "rlQTzK2HujZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300)\n",
        "df_fi = web_checking(period_check=\"14d\", type_deb = \"FI\", keyword_list=dict_all_debtors[\"FI\"], start=51, time_sleep=30)"
      ],
      "metadata": {
        "id": "SNR2xonET3PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika sudah selesai, Disconnect Notebook kemudian connect lagi dan run dari awal"
      ],
      "metadata": {
        "id": "ZkFHRfQ774vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300) #sleep antar baris\n",
        "df_sme = web_checking(period_check=\"14d\", type_deb = \"SME\", keyword_list=dict_all_debtors[\"SME\"], start=1, time_sleep=30) #sleep antar keyword"
      ],
      "metadata": {
        "id": "MBrSDjuDulf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300)\n",
        "df_sme = web_checking(period_check=\"14d\", type_deb = \"SME\", keyword_list=dict_all_debtors[\"SME\"], start=51, time_sleep=30)"
      ],
      "metadata": {
        "id": "yUaK2JfUhUeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300)\n",
        "df_wholesale = web_checking(period_check=\"14d\", type_deb = \"WHOLESALE\", keyword_list=dict_all_debtors[\"WHOLESALE\"], start=1, end=50, time_sleep=30)"
      ],
      "metadata": {
        "id": "2X5lChauhoYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(300)\n",
        "df_wholesale = web_checking(period_check=\"14d\", type_deb = \"WHOLESALE\", keyword_list=dict_all_debtors[\"WHOLESALE\"], start=51, time_sleep=30)"
      ],
      "metadata": {
        "id": "88V_gRAFhhxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}